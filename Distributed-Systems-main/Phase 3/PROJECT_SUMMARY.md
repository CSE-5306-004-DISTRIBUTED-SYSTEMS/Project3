# Project Summary: What We Did and Where

## ğŸ“‹ Quick Overview

We implemented **2PC and Raft consensus algorithms** on top of an existing distributed auction platform **without breaking any original functionality**.

---

## âœ… Requirements Met

| Requirement | Status | Evidence |
|------------|--------|----------|
| Q0: Selected implementation | âœ… | Distributed auction platform (Go + Python) |
| Q1: 2PC Voting Phase | âœ… | Coordinator + 4 participants, proper RPC logging |
| Q2: 2PC Decision Phase | âœ… | Global commit/abort with phase messages |
| Q3: Raft Leader Election | âœ… | 5 nodes, randomized timeouts, heartbeats |
| Q4: Raft Log Replication | âœ… | Client forwarding, majority commit, log application |
| Q5: Test Cases | âœ… | 5 scenarios documented in RAFT_TEST_RESULTS.md |
| Original Systems Intact | âœ… | Both Go and Python auction systems work unchanged |
| Docker Containerization | âœ… | All services dockerized and orchestrated |
| README Documentation | âœ… | Comprehensive README.md with all sections |

---

## ğŸ“ Files We Created (NEW)

```
Phase 3/
â”œâ”€â”€ README.md                    âœ… Comprehensive documentation (all requirements)
â”œâ”€â”€ DEMO_GUIDE.md               âœ… Step-by-step demo instructions
â”œâ”€â”€ docker-compose.yml          âœ… Root orchestration (2PC + Raft + originals)
â”‚
â”œâ”€â”€ proto/                      
â”‚   â”œâ”€â”€ consensus.proto         âœ… 2PC message definitions
â”‚   â””â”€â”€ raft.proto             âœ… Raft message definitions
â”‚
â”œâ”€â”€ pb/                         
â”‚   â”œâ”€â”€ consensus.pb.go         âœ… 2PC Go implementation (manual)
â”‚   â””â”€â”€ raft.pb.go             âœ… Raft Go implementation (manual)
â”‚
â””â”€â”€ services/                   
    â”œâ”€â”€ 2pc_coordinator/        
    â”‚   â””â”€â”€ main.go             âœ… Coordinator logic (150 lines)
    â”œâ”€â”€ 2pc_participant/        
    â”‚   â””â”€â”€ main.go             âœ… Participant logic (120 lines)
    â””â”€â”€ raft_node/              
        â””â”€â”€ main.go             âœ… Complete Raft implementation (392 lines)
```

---

## ğŸ”§ Files We Modified (UPDATED)

```
distributed-auction-system/Distributed-Online-Auction-Platform-main/
â”œâ”€â”€ README.md                   âœ… Added 2PC/Raft sections
â””â”€â”€ go-architecture/
    â””â”€â”€ RAFT_TEST_RESULTS.md    âœ… Complete Q5 test documentation
```

---

## âœ¨ Files Unchanged (ORIGINAL - Still Works!)

```
distributed-auction-system/Distributed-Online-Auction-Platform-main/
â”œâ”€â”€ go-architecture/
â”‚   â”œâ”€â”€ services/               âœ… All 6 Go microservices untouched
â”‚   â”‚   â”œâ”€â”€ aggregator/         
â”‚   â”‚   â”œâ”€â”€ auction/            
â”‚   â”‚   â”œâ”€â”€ bidding/            
â”‚   â”‚   â”œâ”€â”€ history/            
â”‚   â”‚   â”œâ”€â”€ notifier/           
â”‚   â”‚   â””â”€â”€ updates/            
â”‚   â””â”€â”€ docker-compose.yml      âœ… Original Go orchestration
â”‚
â””â”€â”€ python_architecture/
    â”œâ”€â”€ services/               âœ… All 5 Python services untouched
    â”‚   â”œâ”€â”€ frontend/           
    â”‚   â”œâ”€â”€ gateway/            
    â”‚   â”œâ”€â”€ auction_service/    
    â”‚   â”œâ”€â”€ bidding_service/    
    â”‚   â””â”€â”€ history_service/    
    â””â”€â”€ docker-compose.yml      âœ… Original Python orchestration
```

---

## ğŸ¯ What Each Component Does

### 2PC System (Q1-Q2)

**Location**: `services/2pc_coordinator/` and `services/2pc_participant/`  
**Ports**: 7100 (coordinator), 7101-7104 (participants)

**What it does**:
1. Coordinator receives transaction request
2. Broadcasts vote-request to all 4 participants
3. Participants respond with vote-commit or vote-abort
4. Coordinator decides global-commit (if all vote commit) or global-abort (if any vote abort)
5. Coordinator broadcasts decision to all participants
6. Participants commit or abort based on decision

**Key feature**: Proper phase logging per Instructions.txt format

### Raft System (Q3-Q4)

**Location**: `services/raft_node/main.go`  
**Ports**: 7201-7205 (gRPC), 7301-7305 (HTTP)

**What it does**:
1. **Leader Election**: Nodes start as followers, elect leader via randomized timeouts
2. **Heartbeats**: Leader sends AppendEntries every 1 second to maintain leadership
3. **Client Requests**: Any node can receive requests, forwards to leader if necessary
4. **Log Replication**: Leader appends operations to log, replicates to all followers
5. **Commit**: Leader waits for majority ACKs, then commits operation
6. **Apply**: All nodes execute committed operations in order
7. **Failure Recovery**: New leader elected if current leader fails

**Key features**: Fault-tolerant, consistent, automatic recovery

---

## ğŸš€ How to Run Everything

### Quick Start

```powershell
cd "c:\Users\might\OneDrive\Desktop\School\Distributed Systems\Phase 3"
docker compose build
docker compose up -d
```

This starts:
- Go auction system (6 services)
- Python auction system (5 services)
- 2PC cluster (1 coordinator + 4 participants)
- Raft cluster (5 nodes)

### Test 2PC

```powershell
Invoke-RestMethod -Method POST -Uri "http://localhost:7100/auction.TwoPCCoordinator/StartVoting" `
  -ContentType "application/json" `
  -Body '{"tx_id":"test-001","operation":"test-operation"}'

# View logs
docker compose logs 2pc_coordinator
docker compose logs 2pc_participant1
```

### Test Raft

```powershell
# Submit operation
Invoke-RestMethod -Method POST -Uri "http://localhost:7301/client_request" `
  -ContentType "application/json" `
  -Body '{"operation":"my_operation"}'

# View leader election
docker compose logs raft_node1 raft_node2 raft_node3 raft_node4 raft_node5 | Select-String "becomes leader"

# View operation application
docker compose logs raft_node1 raft_node2 raft_node3 raft_node4 raft_node5 | Select-String "applies operation"
```

---

## ğŸ“Š Test Results Summary

All Q5 test cases passed successfully:

1. âœ… **Leader Crash**: n4 stopped â†’ n1 elected â†’ old leader rejoined and synced
2. âœ… **Split Vote**: Cluster restarted simultaneously â†’ n2 elected quickly
3. âœ… **Follower Forwarding**: Request to n5 â†’ forwarded to n2 â†’ applied on all nodes
4. âœ… **Node Join**: n3 offline â†’ 2 ops committed â†’ n3 restarted â†’ synced all 4 ops
5. âœ… **Network Partition**: n2 paused â†’ n5 elected â†’ n2 unpaused â†’ synced new logs

**Evidence**: Complete logs in `go-architecture/RAFT_TEST_RESULTS.md`

---

## ğŸ’¡ Key Technical Decisions

### 1. Custom gRPC Implementation
- **Why**: Hermetic Docker builds without host dependencies
- **Trade-off**: Less features but simpler for this project
- **Location**: `google.golang.org/grpc/grpc.go`

### 2. Manual Protobuf Files
- **Why**: Avoid protoc requirement
- **Trade-off**: Must manually update .pb.go when .proto changes
- **Location**: `pb/consensus.pb.go` and `pb/raft.pb.go`

### 3. Full Log Snapshot in Raft
- **Why**: Per Q4 spec: "sends its entire log"
- **Trade-off**: Less efficient than incremental, but simpler
- **Standard Raft**: Uses prevLogIndex/prevLogTerm for incremental replication

### 4. No Persistent Storage
- **Why**: Focus on consensus logic, not storage
- **Trade-off**: State lost on restart (acceptable for demo)
- **Production**: Would need disk writes for term, votedFor, log

---

## ğŸ› Critical Bugs We Fixed

During testing, we discovered and fixed 7 critical bugs in the Raft implementation:

1. **Vote counting race condition** â†’ Fixed with channel-based synchronization
2. **Index mismatch in applyEntries** â†’ Fixed by matching entry.Index (not array position)
3. **ACK counts not reset** â†’ Fixed by resetting map on leader election
4. **Leader connection lookup failed** â†’ Fixed with connsByID map
5. **commitIndex/lastApplied initialization** â†’ Fixed by starting at -1
6. **Heartbeat data race** â†’ Fixed by passing log snapshots to goroutines
7. **Leader caching incomplete** â†’ Fixed by tracking leaderAddr in AppendEntries

**Result**: System now 100% functional for all test scenarios

---

## ğŸ“– Documentation Files

1. **`README.md`** (root level) - Main documentation
   - How to compile and run
   - Anything unusual about the solution
   - External references
   - Per Instructions.txt requirements

2. **`DEMO_GUIDE.md`** (root level) - Demo walkthrough
   - What we built
   - Where changes were made
   - Step-by-step demo script
   - Technical explanations
   - Common Q&A

3. **`RAFT_TEST_RESULTS.md`** (go-architecture/) - Q5 test results
   - 5 test cases with execution steps
   - Log excerpts showing expected behavior
   - Success criteria verification

---

## âœ… Verification Checklist

Before submission, verify:

- [ ] All services build: `docker compose build`
- [ ] Original Go auction works: Test create/list/bid
- [ ] Original Python auction works: Open http://localhost:8080
- [ ] 2PC voting phase works: Check coordinator logs show "Phase voting"
- [ ] 2PC decision phase works: Check logs show "Phase decision" and commit messages
- [ ] Raft leader election works: Check logs show "becomes leader"
- [ ] Raft log replication works: Submit operation, check all nodes apply it
- [ ] Follower forwarding works: Submit to non-leader port, verify forwarding logs
- [ ] Leader crash recovery works: Stop leader, verify new election
- [ ] README complete: All sections filled (team info, how to run, etc.)
- [ ] GitHub repository up to date: Push all code and documentation

---

## ğŸ“ For Your Report

### What to Include

1. **Team member contributions** (update in README.md)
2. **Screenshots** from RAFT_TEST_RESULTS.md showing:
   - Leader election logs
   - 2PC phase messages
   - Log replication
   - Failure recovery
3. **Architecture diagrams** (already in README)
4. **GitHub link**: https://github.com/mgm67671/Distributed-Systems
5. **How to run** (from README, test commands)
6. **Unusual aspects** (custom gRPC, manual protobuf, full log snapshot)

### Key Points to Emphasize

- âœ… **Original systems unchanged** - demonstrated backward compatibility
- âœ… **All requirements met** - Q1-Q5 fully implemented and tested
- âœ… **Fault tolerance** - leader crash, network partition, node join all handled
- âœ… **Proper logging** - exact format per Instructions.txt specifications
- âœ… **Dockerized** - all 16 services containerized and orchestrated
- âœ… **Well documented** - comprehensive README and demo guide

---

## ğŸ¯ Summary

**Bottom Line**: We successfully implemented 2PC and Raft consensus algorithms on top of an existing distributed auction platform. All original functionality is preserved, all new consensus features work correctly, and everything is fully documented and tested.

**Files to submit**:
- Source code (entire Phase 3 directory)
- README.md (comprehensive documentation)
- DEMO_GUIDE.md (demo walkthrough)
- RAFT_TEST_RESULTS.md (test case evidence)
- Report (PDF with screenshots and explanations)
- GitHub link

**Status**: âœ… **Project Complete and Ready for Submission**
